{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load MNIST dataset\n",
    "from sklearn.datasets import fetch_openml\n",
    "\n",
    "mnist = fetch_openml('mnist_784', version=1)\n",
    "mnist.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **A DESCR** key that describing the dataset\n",
    "- **A data key** containing an array with one row per instance and one column per feature\n",
    "- **A target key** containing an array with the labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# take a look at dataset\n",
    "X, y = mnist[\"data\"], mnist[\"target\"]\n",
    "print(X.shape, y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are **70.000 images**, and each image has **784 features**. This is because each image is **28x28 pixels**. And each feature simply represents one pixel's intensity, from 0 (white) to 255 (black)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display one digit from dataset\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "some_digit = X[0]\n",
    "some_digit_image = some_digit.reshape(28, 28)\n",
    "\n",
    "plt.imshow(some_digit_image, cmap=\"binary\")\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# lets take a look at the label\n",
    "print(y[0])\n",
    "\n",
    "# cast the label into integer type\n",
    "y = y.astype(np.uint8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split dataset into training set and testing set\n",
    "X_train, X_test, y_train, y_test = X[:60000], X[60000:], y[:60000], y[60000:]\n",
    "\n",
    "# since the training set is already shuffled, it will be easy to do cross-validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training a Binary Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We simplify the problem with binary classifier\n",
    "# So, it will distinguish between just two classes, 5 and not-5\n",
    "y_train_5 = (y_train==5) # true for all 5s, false for the other digits\n",
    "y_test_5 = (y_test==5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we will use Stochastic Gradient Descent (SGD)\n",
    "# SGD being capable of deals with very large dataset efficiently\n",
    "\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "sgd_clf = SGDClassifier(random_state=42)\n",
    "sgd_clf.fit(X_train, y_train_5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets use SGD model to predict some digit\n",
    "sgd_clf.predict([some_digit])\n",
    "\n",
    "# the result should be True, since the digit is 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance Measures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Measuring Accuracy Using Cross-Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "cross_val_score(sgd_clf, X_train, y_train_5, cv=3, scoring=\"accuracy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The accuracy results looks good with 96%. But, let's look at very dumb classifier that classifies every single image in the \"not-5\" class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator\n",
    "\n",
    "class Never5Classifier(BaseEstimator):\n",
    "    def fit(self, X, y=None):\n",
    "        pass\n",
    "    def predict(self, X):\n",
    "        return np.zeros((len(X), 1), dtype=bool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's find out the accuracy\n",
    "never_5_clf = Never5Classifier()\n",
    "cross_val_score(never_5_clf, X_train, y_train_5, cv=3, scoring=\"accuracy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result has over 90% accuracy, because only 10% of the images are 5s, so if we always guess that an image is not a 5, we will be right about 90%.\n",
    "\n",
    "This condition called **skewed datasets** (when some classes are much more frequent than others)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion Matrix\n",
    "We must have a set of prediction to be compared to the actual targets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_predict\n",
    "\n",
    "y_train_pred = cross_val_predict(sgd_clf, X_train, y_train_5, cv=3)\n",
    "\n",
    "# cross_val_predict performs k-fold cross-validation\n",
    "# and it returns the predictions made on each test fold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "now let's calculate the confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "confusion_matrix(y_train_5, y_train_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each **row** in a confusion matrix represents an **actual class**, while each **column** represents a **predicted class.**\n",
    "\n",
    "- **53892:** were correctly classified as non-5s **(true negatives)**\n",
    "- **687:** were wrongly classified as 5s **(false positives)**\n",
    "- **1891:** were wrongly classified as non-5s **(false negatives)**\n",
    "- **3530:** were correctly classified as 5s **(true positives)**\n",
    "\n",
    "A perfect classifier would only have **true positives** and **true negatives.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's pretend we reached perfection\n",
    "y_train_perfect_predictions = y_train_5\n",
    "confusion_matrix(y_train_5, y_train_perfect_predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation*}\n",
    "Precision | Accuracy = \\frac{TP}{(TP + FP)}.\n",
    "\\end{equation*}\n",
    "\n",
    "\\begin{equation*}\n",
    "Recall|Sensitivity|True Positive Rate = \\frac{TP}{(TP + FN)}.\n",
    "\\end{equation*}\n",
    "\n",
    "- **TP:** true positive\n",
    "- **FP:** false positive\n",
    "- **FN:** false negative"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Precision and Recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score, recall_score\n",
    "pre = precision_score(y_train_5, y_train_pred)\n",
    "rec = recall_score(y_train_5, y_train_pred)\n",
    "\n",
    "print(pre)\n",
    "print(rec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is convenient to combine precision and recall into a single metric called *F1 score.*\n",
    "\n",
    "\\begin{equation*}\n",
    "F_1 = \\frac{TP}{\\frac{1}{precision} + \\frac{1}{recall}} = 2 \\times \\frac{precision \\times recall}{precision + recall} = \\frac{TP}{TP + \\frac{FN + FP}{2}}.\n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "f1_score(y_train_5, y_train_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In some contexts we mostly care about precision, and in other contexts we really care about recall.\n",
    "\n",
    "For example,\n",
    "- If you trained a classifier to detect videos that are safe for kids, we would probably prefer a classifier that rejects many good videos **(low recall)** but keeps only safe ones **(high precision)** rather than a classifier that has a much higher recall but lets a few really bad videos show up.\n",
    "- Otherwise, suppose we train a classifier to detect shoplifters on surveillance images: its probably fine if your classifier has only 30% precision as long it has 99% recall (sure, the security guards will get a few false alerts, but almost all shoplifters will get caught)\n",
    "\n",
    "We can't have it both ways: increasing precision reduces recall, and vice versa. This is called **precision/recall tradeoff**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Precision/Recall Tradeoff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scikit-learn gives access to the decision scores that it uses to make predictions\n",
    "# decision_function() method returns a score for each instance, and then make predictions based on those score using any threshold\n",
    "y_scores = sgd_clf.decision_function([some_digit])\n",
    "print(y_scores)\n",
    "\n",
    "threshold = 0\n",
    "y_some_digit_pred = (y_scores > threshold)\n",
    "print(y_some_digit_pred)\n",
    "\n",
    "# it will return the same result as the predict() method -> True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's raise the threshold\n",
    "threshold = 8000\n",
    "y_some_digit_pred = (y_scores > threshold)\n",
    "print(y_some_digit_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Raising the threshold decreases recall. The image actually represents a 5, and the classifier detects it when the threshold is 0, but it misses it when the threshold is increased to 8000.\n",
    "\n",
    "Now, how do we decide which threshold to use?\n",
    "We need to get the scores of all instances in the training set using cross_val_predict() function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_scores = cross_val_predict(sgd_clf, X_train, y_train_5, cv=3,\n",
    "                             method=\"decision_function\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now with these scores, we can compute precision and recall for all possible thresholds using *precision_recall_curve*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_curve\n",
    "\n",
    "precisions, recalls, thresholds = precision_recall_curve(y_train_5, y_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot precision and recall\n",
    "def plot_precision_recall_vs_threshold(precisions, recalls, thresholds):\n",
    "    plt.plot(thresholds, precisions[:-1], \"b--\", label=\"Precision\")\n",
    "    plt.plot(thresholds, recalls[:-1], \"g-\", label=\"Recall\")\n",
    "    plt.legend(loc=\"center right\", fontsize=16)\n",
    "    plt.xlabel(\"Threshold\", fontsize=16)\n",
    "    plt.grid(True)\n",
    "    plt.axis([-50000, 50000, 0, 1])\n",
    "\n",
    "plot_precision_recall_vs_threshold(precisions, recalls, thresholds)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Precision curve is bumpier than recall curve, because the precision may sometimes go down when we raise the threshold. On the other hand, recall can only go down when the threshold is increased, which explains why its curve looks smooth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# precision/recall trade-off depend on our project\n",
    "\n",
    "# let's try with 90% precision\n",
    "threshold_90_precision = thresholds[np.argmax(precisions >= 0.90)]\n",
    "threshold_90_precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make predictions\n",
    "y_train_pred_90 = (y_scores >= threshold_90_precision)\n",
    "y_train_pred_90"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check precision and recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(precision_score(y_train_5, y_train_pred_90))\n",
    "print(recall_score(y_train_5, y_train_pred_90))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If someone says \"let's reach 99% precision\", you should ask, \"at what recall?\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The ROC Curve\n",
    "*Receiving Operating Characteristics (ROC)* curve plotting precision versus recall. ROC curve plot the *true positive rate (recall)* against the *false positive rate.*\n",
    "\n",
    "The ROC curve plots *sensitivity* (recall) versus 1 - *specificity* "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve\n",
    "\n",
    "fpr, tpr, thresholds = roc_curve(y_train_5, y_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's plot roc curve\n",
    "def plot_roc_curve(fpr, tpr, label=None):\n",
    "    plt.plot(fpr, tpr, linewidth=2, label=label)\n",
    "    plt.plot([0,1], [0,1], 'k--')\n",
    "    plt.axis([0, 1, 0, 1])\n",
    "    plt.xlabel('False Positive Rate (Fall-Out)', fontsize=16)\n",
    "    plt.ylabel('True Positive Rate (Recall)', fontsize=16)\n",
    "    plt.grid(True)\n",
    "    \n",
    "plot_roc_curve(fpr, tpr)\n",
    "plt.show()\n",
    "\n",
    "# There is a tradeoff: the higher TPR, the more FPR the classifier produces\n",
    "# The dotted line represents ROC curve of a purely random classifier\n",
    "# A good classifier stays as far away from that line (toward the top-left corner)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One way to compare classifiers is to measure the *area under the curve* (AUC). A perfect classifier will have ROC AUC equal to 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "roc_auc_score(y_train_5, y_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How to decide which one to use ROC curve or PR curve?\n",
    "- We should prefer PR curve whenever the positive class is rare or when we care more about false positives than the false negatives.\n",
    "- And the ROC curve otherwise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's train a RandomForestClassifier and compare its ROC curve and ROC AUC score to the SGDClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "forest_clf = RandomForestClassifier(random_state=42)\n",
    "y_probas_forest = cross_val_predict(forest_clf, X_train, y_train_5, cv=3,\n",
    "                                    method=\"predict_proba\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to plot a ROC curve, we need scores\n",
    "y_scores_forest = y_probas_forest[:, 1] # proba of positive class\n",
    "fpr_forest, tpr_forest, threshold_forest = roc_curve(y_train_5, y_scores_forest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the ROC curve\n",
    "plt.plot(fpr, tpr, \"b:\", label=\"SGD\")\n",
    "plot_roc_curve(fpr_forest, tpr_forest, \"Random Forest\")\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, *RandomForestClassifier* is better than *SGDClassifier*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's calculate the roc auc score\n",
    "print(roc_auc_score(y_train_5, y_scores_forest))\n",
    "\n",
    "# precision and recall\n",
    "y_train_pred_forest = cross_val_predict(forest_clf, X_train, y_train_5, cv=3)\n",
    "print(precision_score(y_train_5, y_train_pred_forest))\n",
    "print(recall_score(y_train_5, y_train_pred_forest))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multiclass Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- SGD classifiers, Random Forest classifiers, and naive Bayes classifiers are capable of handling multiple classes.\n",
    "- Logistic Regression and Support Vector Machine classifiers are strictly binary classifiers.\n",
    "\n",
    "There are various strategies to perform multiclass classification using multiple binary classifiers.\n",
    "- **OvA (one-versus-the-rest):** create a system that can classify the digit images into 10 classes (from 0 to 9) is to train 10 binary classifiers, one for each digit. So, when we want to classify an image, we get the decision scores from each classifier for that image and select the class whose classifier outputs the highest score.\n",
    "- **OvO (one-versus-one):** train a binary classifier for every pair of digits: one to distinguish 0s and 1s, etc. If there is N classes, we need to train *N x (N - 1)/2* classifiers. The advantage is that each classifier only needs to be trained on the part of the training set for the two classes that it must be distinguish.\n",
    "\n",
    "For most binary classification algorithms, OvA is preferred."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fortunately, scikit-learn can automatically runs OvA or OvR, depending o the algorithm\n",
    "from sklearn.svm import SVC\n",
    "svm_clf = SVC()\n",
    "svm_clf.fit(X_train, y_train)\n",
    "svm_clf.predict([some_digit])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# actually scikit-learn used the OvO strategy:\n",
    "# it trained binary classifiers, got their decision scores for the image,\n",
    "# and selected the class that won the most duels\n",
    "\n",
    "# let's show the decision scores with decision_function\n",
    "some_digit_scores = svm_clf.decision_function([some_digit])\n",
    "some_digit_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The highest score:\n",
    "print(np.argmax(some_digit_scores))\n",
    "\n",
    "print(svm_clf.classes_)\n",
    "print(svm_clf.classes_[5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can force scikit-learn to use OvO or OvA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "ovr_clf = OneVsRestClassifier(SVC())\n",
    "ovr_clf.fit(X_train, y_train)\n",
    "print(ovr_clf.predict([some_digit]))\n",
    "\n",
    "print(len(ovr_clf.estimators_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training with SGDClassifier or RandomForestClassifier is just as easy\n",
    "sgd_clf.fit(X_train, y_train)\n",
    "sgd_clf.predict([some_digit])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In SGDClassifier, scikit-learn did not have to run OvA or OvO because SGD classifiers can directly classify instances into multiple classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's look at the score that the SGD classifier assigned\n",
    "sgd_clf.decision_function([some_digit])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Class #5 has a score of 2412.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate SGDClassifier\n",
    "cross_val_score(sgd_clf, X_train, y_train, cv=3, scoring=\"accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can optimize the result by scaling the inputs\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train.astype(np.float64))\n",
    "cross_val_score(sgd_clf, X_train_scaled, y_train, cv=3, scoring=\"accuracy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Error Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's take a look at the confusion matrix\n",
    "y_train_pred = cross_val_predict(sgd_clf, X_train_scaled, y_train, cv=3)\n",
    "conf_mx = confusion_matrix(y_train, y_train_pred)\n",
    "conf_mx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the confusion matrix\n",
    "plt.matshow(conf_mx, cmap=plt.cm.gray)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to divide each value in the confusion matrix by the numbe of images in the corresponding class, so we can compare error rates instead absolute number of errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "row_sums = conf_mx.sum(axis=1, keepdims=True)\n",
    "norm_conf_mx = conf_mx/row_sums"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now let's fill the diagonal with zeros to keep only the errors\n",
    "np.fill_diagonal(norm_conf_mx, 0)\n",
    "plt.matshow(norm_conf_mx, cmap=plt.cm.gray)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Remember that rows represent actual classes, while columns represent predicted classes.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analyzing individual errors can also be a good way to gain insights on what your classifier is doing and why it is failing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's plot examples of 3s and 5s\n",
    "cl_a, cl_b = 3, 5\n",
    "X_aa = X_train[(y_train == cl_a) & (y_train_pred == cl_a)]\n",
    "X_ab = X_train[(y_train == cl_a) & (y_train_pred == cl_b)]\n",
    "X_ba = X_train[(y_train == cl_b) & (y_train_pred == cl_a)]\n",
    "X_bb = X_train[(y_train == cl_b) & (y_train_pred == cl_b)]\n",
    "\n",
    "plt.figure(figsize=(8,8))\n",
    "plt.subplot(221); plot_digits(X_aa[:25], images_per_row=5)\n",
    "plt.subplot(222); plot_digits(X_ab[:25], images_per_row=5)\n",
    "plt.subplot(223); plot_digits(X_ba[:25], images_per_row=5)\n",
    "plt.subplot(224); plot_digits(X_bb[:25], images_per_row=5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The two 5x5 blocks on the left show digits classifier as 3s, and the two 5x5 blocks on the right show images classified as 5s. Some of the digits that the classifier gets wrong.\n",
    "\n",
    "The reason is that we used a simple SGDClassifier, which is linear model. All it does is assign a weight per class to each pixel, and when it sees a new image it just sums up the weighted pixel intensities to get a score for each class. So since 3s and 5s differ only by a few pixels, this model will easily confuse them.\n",
    "\n",
    "One way to reduce the 3/5 confuson would be to preprocess the images to ensure that they are well centered and not too rotated. This will probably help reduce other errors as well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multilabel Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In some cases, we want our classifier to output multiple classes for each instances.\n",
    "\n",
    "Example:\n",
    "- There is a classifier that has been trained to recognize three faces, Alice, Bob, and Charlie.\n",
    "- When it is shown a picture of Alice and Charlie, it should output (1, 0, 1) meaning \"Alice yes, Bob no, Charlie yes\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "y_train_large = (y_train >= 7)\n",
    "y_train_odd = (y_train % 2 == 1)\n",
    "y_multilabel = np.c_[y_train_large, y_train_odd]\n",
    "\n",
    "knn_clf = KNeighborsClassifier()\n",
    "knn_clf.fit(X_train, y_multilabel)\n",
    "\n",
    "# the first indicates whether or not the digit is large (7, 8, 9)\n",
    "# the second indicates whether or not it is odd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now we can make a prediction\n",
    "knn_clf.predict([some_digit])\n",
    "\n",
    "# the result is, the digit 5 is not large (False) and odd (True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can measure it using F1 score for each individual label\n",
    "y_train_knn_pred = cross_val_predict(knn_clf, X_train, y_multilabel, cv=3)\n",
    "f1_score(y_multilabel, y_train_knn_pred, average=\"macro\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multioutput Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is a simply a generalization of multilabel classification where each label can be multiclass (i.e., it can have more than two possible values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's create traning and test sets and adding noise to their pixel intensities\n",
    "noise = np.random.randint(0, 100, (len(X_train), 784))\n",
    "X_train_mod = X_train + noise\n",
    "noise = np.random.randint(0, 100, (len(X_test), 784))\n",
    "y_test_mod = X_test + mod\n",
    "y_train_mod = X_train\n",
    "y_test_mod = X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's take a peek\n",
    "some_index = 0\n",
    "plt.subplot(121); plot_digit(X_test_mod[some_index])\n",
    "plt.subplot(122); plot_digit(y_test_mod[some_index])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's train the classifier and make it clean \n",
    "knn_clf.fit(X_train_mod, y_train_mod)\n",
    "clean_digit = knn_clf.predict([X_test_mod[some_index]])\n",
    "plot_digit(clean_digit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tf_env]",
   "language": "python",
   "name": "conda-env-tf_env-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
